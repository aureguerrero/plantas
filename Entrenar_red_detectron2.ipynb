{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Entrenar red detectron2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aureguerrero/plantas/blob/main/Entrenar_red_detectron2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFx84wjASeRh"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from numpy.ma.core import sort\n",
        "import os\n",
        "import shutil\n",
        "!git clone https://github.com/aureguerrero/plantas.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargamos los zip"
      ],
      "metadata": {
        "id": "P6UDZTZ8njgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir arch_zip\n",
        "buffer='n' #y/n\n",
        "np.save('condicion.npy',buffer)"
      ],
      "metadata": {
        "id": "LpJIW4EXgtxE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train={}\n",
        "data_test={}\n",
        "for archivo in os.listdir('arch_zip'):\n",
        "  with ZipFile('/content/arch_zip/'+archivo, 'r') as zip:\n",
        "    zip.printdir()\n",
        "    zip.extractall()\n",
        "  print(len(data_train))\n",
        "  data=data_train.copy()\n",
        "  with open('/content/content/plantas/conj_ima_new/train/numpy_train_new.csv') as csvFile:\n",
        "    csvread=csv.DictReader(csvFile)\n",
        "    nombre_viejo='2'\n",
        "    for rows in csvread:\n",
        "  #    print(rows)\n",
        "      if rows['filename'] != nombre_viejo:\n",
        "        data[rows['filename']+rows['file_size']]={}\n",
        "        data[rows['filename']+rows['file_size']]={'filename': rows['filename'],'size': rows['file_size'],'regions': [{'shape_attributes':json.loads( rows['region_shape_attributes']),'region_attributes': {'obj': 'planta'}}]}\n",
        "        nombre_viejo=rows['filename']\n",
        "      else:\n",
        "        data[rows['filename']+rows['file_size']]['regions'].append({ 'shape_attributes':json.loads( rows['region_shape_attributes']),'region_attributes': {'obj': 'planta'}})\n",
        "  data_train=data.copy()\n",
        "  print(len(data_train))  \n",
        "\n",
        "  print(len(data_test))\n",
        "  data=data_test.copy()\n",
        "  with open('/content/content/plantas/conj_ima_new/val/numpy_test_new.csv') as csvFile:\n",
        "    csvread=csv.DictReader(csvFile)\n",
        "    nombre_viejo='2'\n",
        "    for rows in csvread:\n",
        "  #    print(rows)\n",
        "      if rows['filename'] != nombre_viejo:\n",
        "        data[rows['filename']+rows['file_size']]={}\n",
        "        data[rows['filename']+rows['file_size']]={'filename': rows['filename'],'size': rows['file_size'],'regions': [{'shape_attributes':json.loads( rows['region_shape_attributes']),'region_attributes': {'obj': 'planta'}}]}\n",
        "        nombre_viejo=rows['filename']\n",
        "      else:\n",
        "        data[rows['filename']+rows['file_size']]['regions'].append({ 'shape_attributes':json.loads( rows['region_shape_attributes']),'region_attributes': {'obj': 'planta'}})\n",
        "  data_test=data.copy()\n",
        "  print(len(data_test))\n",
        "\n",
        "if buffer=='y':\n",
        "  %cd /content/content/plantas/conj_ima_new\n",
        "  %cp dji*.* /content/plantas/conj_ima\n",
        "  %cd /content/content/plantas/conj_ima_new/train\n",
        "  %cp dji*.* /content/plantas/train\n",
        "  %cd /content/content/plantas/conj_ima_new/val\n",
        "  %cp dji*.* /content/plantas/val\n",
        "  test=[_ for _ in sort(os.listdir('/content/plantas/val')) if _.endswith('npy') and _.startswith('dji')]\n",
        "  train=[_ for _ in sort(os.listdir('/content/plantas/train')) if _.endswith('npy')and _.startswith('dji')]\n",
        "  at=[['filename', 'file_size', 'file_attributes', 'region_count', 'region_id', 'region_shape_attributes', 'region_attributes']]\n",
        "  for i in test:\n",
        "    a=np.load(\"/content/plantas/val/\"+i, allow_pickle=True)\n",
        "    filename=i[:-3]+'png'\n",
        "    file_size=str(os.path.getsize(r'/content/plantas/val/'+i[:-3]+'png'))\n",
        "    file_attributes='\"{}\"'\n",
        "    region_count=str(len(a))\n",
        "    for j in range(0,len(a)):\n",
        "      region_id=str(j)\n",
        "      region_shape_attributes='\"{\"\"name\"\":\"\"polygon\"\",\"\"all_points_x\"\":['+','.join(str(np.int16(x)) for x in a[j][:,0])+'],\"\"all_points_y\"\":['+','.join(str(np.int16(x)) for x in a[j][:,1])+']}\"'\n",
        "      region_attributes='\"\"obj\"\":\"\"planta\"\"'\n",
        "      at.append([filename, file_size, file_attributes, region_count, region_id, region_shape_attributes, region_attributes])\n",
        "\n",
        "  at2=[['filename', 'file_size', 'file_attributes', 'region_count', 'region_id', 'region_shape_attributes', 'region_attributes']]\n",
        "\n",
        "  for i in train:\n",
        "    a=np.load(\"/content/plantas/train/\"+i, allow_pickle=True)\n",
        "    filename=i[:-3]+'png'\n",
        "    file_size=str(os.path.getsize(r'/content/plantas/train/'+i[:-3]+'png'))\n",
        "    file_attributes='\"{}\"'\n",
        "    region_count=str(len(a))\n",
        "    for j in range(0,len(a)):\n",
        "      region_id=str(j)\n",
        "      region_shape_attributes='\"{\"\"name\"\":\"\"polygon\"\",\"\"all_points_x\"\":['+','.join(str(np.int16(x)) for x in a[j][:,0])+'],\"\"all_points_y\"\":['+','.join(str(np.int16(x)) for x in a[j][:,1])+']}\"'\n",
        "      region_attributes='\"\"obj\"\":\"\"planta\"\"'\n",
        "      at2.append([filename, file_size, file_attributes, region_count, region_id, region_shape_attributes, region_attributes])\n",
        "\n",
        "\n",
        "  np.savetxt(\"/content/plantas/val/numpy_test.csv\", at, delimiter =\",\",fmt ='% s')\n",
        "  np.savetxt(\"/content/plantas/train/numpy_train.csv\", at2, delimiter =\",\",fmt ='% s')\n",
        "  %cp /content/plantas/train/*.csv /content/content/plantas/conj_ima_new/train\n",
        "  %cp /content/plantas/val/*.csv /content/content/plantas/conj_ima_new/val\n",
        "  data={}\n",
        "  with open('/content/plantas/val/numpy_test.csv') as csvFile:\n",
        "    csvread=csv.DictReader(csvFile)\n",
        "    nombre_viejo='2'\n",
        "    for rows in csvread:\n",
        "  #    print(rows)\n",
        "      if rows['filename'] != nombre_viejo:\n",
        "        data[rows['filename']+rows['file_size']]={}\n",
        "        data[rows['filename']+rows['file_size']]={'filename': rows['filename'],'size': rows['file_size'],'regions': [{'shape_attributes':json.loads( rows['region_shape_attributes']),'region_attributes': {'obj': 'planta'}}]}\n",
        "        nombre_viejo=rows['filename']\n",
        "      else:\n",
        "        data[rows['filename']+rows['file_size']]['regions'].append({ 'shape_attributes':json.loads( rows['region_shape_attributes']),'region_attributes': {'obj': 'planta'}})\n",
        "  print(len(data))\n",
        "  data_test=data.copy()\n",
        "  data={}\n",
        "  with open('/content/plantas/train/numpy_train.csv') as csvFile:\n",
        "    csvread=csv.DictReader(csvFile)\n",
        "    nombre_viejo='2'\n",
        "    for rows in csvread:\n",
        "  #    print(rows)\n",
        "      if rows['filename'] != nombre_viejo:\n",
        "        data[rows['filename']+rows['file_size']]={}\n",
        "        data[rows['filename']+rows['file_size']]={'filename': rows['filename'],'size': rows['file_size'],'regions': [{'shape_attributes':json.loads( rows['region_shape_attributes']),'region_attributes': {'obj': 'planta'}}]}\n",
        "        nombre_viejo=rows['filename']\n",
        "      else:\n",
        "        data[rows['filename']+rows['file_size']]['regions'].append({ 'shape_attributes':json.loads( rows['region_shape_attributes']),'region_attributes': {'obj': 'planta'}})\n",
        "  print(len(data))\n",
        "  data_train=data.copy()\n",
        "  with open('/content/plantas/train/via_json.json', 'w') as fp:\n",
        "    json.dump(data_train, fp)\n",
        "  %cp /content/plantas/train/*.json /content/content/plantas/conj_ima_new/train\n",
        "  with open('/content/plantas/val/via_json.json', 'w') as fp:\n",
        "    json.dump(data_test, fp)\n",
        "  %cp /content/plantas/val/*.json /content/content/plantas/conj_ima_new/val\n",
        "\n",
        "\n",
        "else:\n",
        "  %cd /content/content/plantas/conj_ima_new\n",
        "  %cp dji*.* /content/plantas/sinbuffer/conj_ima\n",
        "  %cd /content/content/plantas/conj_ima_new/train\n",
        "  %cp dji*.* /content/plantas/sinbuffer/train\n",
        "  %cd /content/content/plantas/conj_ima_new/val\n",
        "  %cp dji*.* /content/plantas/sinbuffer/val\n",
        "  test=[_ for _ in sort(os.listdir('/content/plantas/sinbuffer/val')) if _.endswith('npy') and _.startswith('dji')]\n",
        "  train=[_ for _ in sort(os.listdir('/content/plantas/sinbuffer/train')) if _.endswith('npy')and _.startswith('dji')]\n",
        "  at=[['filename', 'file_size', 'file_attributes', 'region_count', 'region_id', 'region_shape_attributes', 'region_attributes']]\n",
        "  for i in test:\n",
        "    a=np.load(\"/content/plantas/sinbuffer/val/\"+i, allow_pickle=True)\n",
        "    filename=i[:-3]+'png'\n",
        "    file_size=str(os.path.getsize(r'/content/plantas/sinbuffer/val/'+i[:-3]+'png'))\n",
        "    file_attributes='\"{}\"'\n",
        "    region_count=str(len(a))\n",
        "    for j in range(0,len(a)):\n",
        "      region_id=str(j)\n",
        "      region_shape_attributes='\"{\"\"name\"\":\"\"polygon\"\",\"\"all_points_x\"\":['+','.join(str(np.int16(x)) for x in a[j][:,0])+'],\"\"all_points_y\"\":['+','.join(str(np.int16(x)) for x in a[j][:,1])+']}\"'\n",
        "      region_attributes='\"\"obj\"\":\"\"planta\"\"'\n",
        "      at.append([filename, file_size, file_attributes, region_count, region_id, region_shape_attributes, region_attributes])\n",
        "\n",
        "  at2=[['filename', 'file_size', 'file_attributes', 'region_count', 'region_id', 'region_shape_attributes', 'region_attributes']]\n",
        "\n",
        "  for i in train:\n",
        "    a=np.load(\"/content/plantas/sinbuffer/train/\"+i, allow_pickle=True)\n",
        "    filename=i[:-3]+'png'\n",
        "    file_size=str(os.path.getsize(r'/content/plantas/sinbuffer/train/'+i[:-3]+'png'))\n",
        "    file_attributes='\"{}\"'\n",
        "    region_count=str(len(a))\n",
        "    for j in range(0,len(a)):\n",
        "      region_id=str(j)\n",
        "      region_shape_attributes='\"{\"\"name\"\":\"\"polygon\"\",\"\"all_points_x\"\":['+','.join(str(np.int16(x)) for x in a[j][:,0])+'],\"\"all_points_y\"\":['+','.join(str(np.int16(x)) for x in a[j][:,1])+']}\"'\n",
        "      region_attributes='\"\"obj\"\":\"\"planta\"\"'\n",
        "      at2.append([filename, file_size, file_attributes, region_count, region_id, region_shape_attributes, region_attributes])\n",
        "\n",
        "\n",
        "  np.savetxt(\"/content/plantas/sinbuffer/val/numpy_test.csv\", at, delimiter =\",\",fmt ='% s')\n",
        "  np.savetxt(\"/content/plantas/sinbuffer/train/numpy_train.csv\", at2, delimiter =\",\",fmt ='% s')\n",
        "  %cp /content/plantas/sinbuffer/train/*.csv /content/content/plantas/conj_ima_new/train\n",
        "  %cp /content/plantas/sinbuffer/val/*.csv /content/content/plantas/conj_ima_new/val\n",
        "  data={}\n",
        "  with open('/content/plantas/sinbuffer/val/numpy_test.csv') as csvFile:\n",
        "    csvread=csv.DictReader(csvFile)\n",
        "    nombre_viejo='2'\n",
        "    for rows in csvread:\n",
        "  #    print(rows)\n",
        "      if rows['filename'] != nombre_viejo:\n",
        "        data[rows['filename']+rows['file_size']]={}\n",
        "        data[rows['filename']+rows['file_size']]={'filename': rows['filename'],'size': rows['file_size'],'regions': [{'shape_attributes':json.loads( rows['region_shape_attributes']),'region_attributes': {'obj': 'planta'}}]}\n",
        "        nombre_viejo=rows['filename']\n",
        "      else:\n",
        "        data[rows['filename']+rows['file_size']]['regions'].append({ 'shape_attributes':json.loads( rows['region_shape_attributes']),'region_attributes': {'obj': 'planta'}})\n",
        "  print(len(data))\n",
        "  data_test=data.copy()\n",
        "  data={}\n",
        "  with open('/content/plantas/sinbuffer/train/numpy_train.csv') as csvFile:\n",
        "    csvread=csv.DictReader(csvFile)\n",
        "    nombre_viejo='2'\n",
        "    for rows in csvread:\n",
        "  #    print(rows)\n",
        "      if rows['filename'] != nombre_viejo:\n",
        "        data[rows['filename']+rows['file_size']]={}\n",
        "        data[rows['filename']+rows['file_size']]={'filename': rows['filename'],'size': rows['file_size'],'regions': [{'shape_attributes':json.loads( rows['region_shape_attributes']),'region_attributes': {'obj': 'planta'}}]}\n",
        "        nombre_viejo=rows['filename']\n",
        "      else:\n",
        "        data[rows['filename']+rows['file_size']]['regions'].append({ 'shape_attributes':json.loads( rows['region_shape_attributes']),'region_attributes': {'obj': 'planta'}})\n",
        "  print(len(data))\n",
        "  data_train=data.copy()\n",
        "  with open('/content/plantas/sinbuffer/train/via_json.json', 'w') as fp:\n",
        "    json.dump(data_train, fp)\n",
        "  %cp /content/plantas/sinbuffer/train/*.json /content/content/plantas/conj_ima_new/train\n",
        "  with open('/content/plantas/sinbuffer/val/via_json.json', 'w') as fp:\n",
        "    json.dump(data_test, fp)\n",
        "  %cp /content/plantas/sinbuffer/val/*.json /content/content/plantas/conj_ima_new/val  \n"
      ],
      "metadata": {
        "id": "jPycJVL2g9Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Detectron2"
      ],
      "metadata": {
        "id": "oRIB0mzVoTKs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsePPpwZSmqt"
      },
      "outputs": [],
      "source": [
        "!pip install pyyaml==5.1\n",
        "\n",
        "import torch\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "# Install detectron2 that matches the above pytorch version\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html\n",
        "# If there is not yet a detectron2 release that matches the given torch + CUDA version, you need to install a different pytorch.\n",
        "\n",
        "exit(0)  # After installation, you may need to \"restart runtime\" in Colab. This line can also restart runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyAvNCJMmvFF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "buffer=np.load('condicion.npy')\n",
        "str(buffer)\n",
        "import json\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from numpy.ma.core import sort\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.structures import BoxMode\n",
        "\n",
        "if buffer=='y':\n",
        "  files_conj_ima=\"/content/plantas/\"\n",
        "else:\n",
        "  files_conj_ima=\"/content/plantas/sinbuffer/\"\n",
        "\n",
        "def get_planta_dicts(img_dir):\n",
        "    json_file = os.path.join(img_dir, \"via_json.json\")\n",
        "    with open(json_file) as f:\n",
        "        imgs_anns = json.load(f)\n",
        "\n",
        "    dataset_dicts = []\n",
        "    for idx, v in enumerate(imgs_anns.values()):\n",
        "        record = {}\n",
        "        \n",
        "        filename = os.path.join(img_dir, v[\"filename\"])\n",
        "        height, width = cv2.imread(filename).shape[:2]\n",
        "        \n",
        "        record[\"file_name\"] = filename\n",
        "        record[\"image_id\"] = idx\n",
        "        record[\"height\"] = height\n",
        "        record[\"width\"] = width\n",
        "      \n",
        "        annos = v[\"regions\"]\n",
        "        objs = []\n",
        "        for anno in annos:\n",
        "#            assert not anno[\"region_attributes\"]\n",
        "            anno = anno[\"shape_attributes\"]\n",
        "            px = anno[\"all_points_x\"]\n",
        "            py = anno[\"all_points_y\"]\n",
        "            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
        "            poly = [p for x in poly for p in x]\n",
        "\n",
        "            obj = {\n",
        "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
        "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                \"segmentation\": [poly],\n",
        "                \"category_id\": 0,\n",
        "            }\n",
        "            objs.append(obj)\n",
        "        record[\"annotations\"] = objs\n",
        "        dataset_dicts.append(record)\n",
        "    return dataset_dicts\n",
        "\n",
        "for d in [\"train\", \"val\"]:\n",
        "    DatasetCatalog.register(\"planta_\" + d, lambda d=d: get_planta_dicts(files_conj_ima + d))\n",
        "    MetadataCatalog.get(\"planta_\" + d).set(thing_classes=[\"planta\"])\n",
        "planta_train_metadata = MetadataCatalog.get(\"planta_train\")\n",
        "planta_val_metadata = MetadataCatalog.get(\"planta_val\")\n",
        "dataset_dicts = get_planta_dicts(files_conj_ima+\"val\")\n",
        "for d in random.sample(dataset_dicts, 3):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=planta_val_metadata, scale=1.)\n",
        "    out2 = visualizer.draw_dataset_dict(d)\n",
        "    cv2_imshow(out2.get_image()[:, :, ::-1])\n",
        "    del img,visualizer,out2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Por si hay que corregir la celda anterior\n",
        "# DatasetCatalog.remove('planta_val')\n",
        "# DatasetCatalog.remove('planta_')"
      ],
      "metadata": {
        "id": "UWT2nOuAneMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/Shareddrives/Conteo de plantas/SEMANTICA/MAIZ/detectron2\n",
        "\n",
        "from detectron2.engine import DefaultTrainer\n",
        "n_itera: int =2000\n",
        "\n",
        "cfg = get_cfg()\n",
        "#cfg.merge_from_file(os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\"))\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"planta_train\",)\n",
        "cfg.DATASETS.TEST = (\"planta_val\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = (os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\"))\n",
        "#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
        "cfg.SOLVER.MAX_ITER = n_itera    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
        "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
        "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg) \n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n",
        "\n",
        "#guardado de archivos\n",
        "if buffer=='y':\n",
        "  shutil.copy('/content/drive/Shareddrives/Conteo de plantas/SEMANTICA/MAIZ/detectron2/output/last_checkpoint', '/content/drive/Shareddrives/Conteo de plantas/SEMANTICA/MAIZ/detectron2/output/conbuffer/last_checkpoint')\n",
        "  shutil.copy('/content/drive/Shareddrives/Conteo de plantas/SEMANTICA/MAIZ/detectron2/output/metrics.json', '/content/drive/Shareddrives/Conteo de plantas/SEMANTICA/MAIZ/detectron2/output/conbuffer/metrics.json')\n",
        "  shutil.copy('/content/drive/Shareddrives/Conteo de plantas/SEMANTICA/MAIZ/detectron2/output/model_final.pth', '/content/drive/Shareddrives/Conteo de plantas/SEMANTICA/MAIZ/detectron2/output/conbuffer/model_final.pth')\n",
        "else:\n",
        "  shutil.copy('/content/drive/Shareddrives/Conteo de plantas/SEMANTICA/MAIZ/detectron2/output/last_checkpoint', '/content/drive/Shareddrives/Conteo de plantas/SEMANTICA/MAIZ/detectron2/output/sinbuffer/last_checkpoint')\n",
        "  shutil.copy('/content/drive/Shareddrives/Conteo de plantas/SEMANTICA/MAIZ/detectron2/output/metrics.json', '/content/drive/Shareddrives/Conteo de plantas/SEMANTICA/MAIZ/detectron2/output/sinbuffer/metrics.json')\n",
        "  shutil.copy('/content/drive/Shareddrives/Conteo de plantas/SEMANTICA/MAIZ/detectron2/output/model_final.pth', '/content/drive/Shareddrives/Conteo de plantas/SEMANTICA/MAIZ/detectron2/output/sinbuffer/model_final.pth')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w7Vk5bPirswZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output\n"
      ],
      "metadata": {
        "id": "I0-rN4ViJ8wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#inferencia sobre validaci칩n\n",
        "\n",
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "cfg = get_cfg()\n",
        "if buffer=='y':\n",
        "  cfg.OUTPUT_DIR='./output/conbuffer'\n",
        "else:\n",
        "  cfg.OUTPUT_DIR='./output/sinbuffer'\n",
        "#cfg.merge_from_file(os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\"))\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"planta_train\",)\n",
        "cfg.DATASETS.TEST = ()#(\"planta_val\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = (os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\"))\n",
        "#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
        "cfg.SOLVER.MAX_ITER = 1000    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
        "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg.MODEL.DEVICE='cpu'\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
        "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
        "\n",
        "# Inference should use the config with parameters that are used in training\n",
        "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "dataset_dicts = get_planta_dicts(files_conj_ima+\"val\")\n",
        "for d in dataset_dicts:    \n",
        "    print(d[\"file_name\"])\n",
        "    print('Verdad vs Predicci칩n')\n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(im[:, :, ::-1], metadata=planta_val_metadata, scale=1.)\n",
        "    out2 = visualizer.draw_dataset_dict(d)\n",
        "    \n",
        "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=planta_val_metadata, \n",
        "                   scale=1.0, \n",
        "                   instance_mode=ColorMode.IMAGE_BW,\n",
        "                   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
        "    )\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    hor = np.hstack((out2.get_image()[:, :, ::-1], np.zeros((im.shape[0],20,3))+255, out.get_image()[:, :, ::-1]))\n",
        "    cv2_imshow(hor)\n",
        "    del im,visualizer,out2,outputs,v,out,hor\n",
        "    \n",
        "\n",
        "#guardado de archivos\n",
        "\n",
        "%cd /content/content/plantas\n",
        "!zip -r /content/drive/Shareddrives/Conteo de plantas/SEMANTICA/INPUT DATA SETS/Ima패genes + Vectorizacio패n + Tests/5_agithub/conj_ima_new.zip conj_ima_new/\n"
      ],
      "metadata": {
        "id": "TtGk2ljJKFBe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}